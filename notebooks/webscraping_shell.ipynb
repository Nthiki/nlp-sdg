{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "from newspaper import Config\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_file(index_file):\n",
    "    \"\"\"Generate a list of webpages URL files that need to be processed. \n",
    "\n",
    "    Args:\n",
    "        index_file (str): index file\n",
    "\n",
    "    Returns:\n",
    "        list: URLs of Webpages. \n",
    "    \"\"\"\n",
    "    \n",
    "    webpage_file = open(index_file_path, \"r\")\n",
    "    contents = webpage_file.read()\n",
    "    contents = contents.replace(\"'\",\"\")\n",
    "    contents_list = contents.split(\",\")\n",
    "    cleaned_contents_list = [item.strip() for item in contents_list]\n",
    "    webpage_file.close()\n",
    "    return cleaned_contents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file directories\n",
    "# Testing environment needs.\n",
    "source_path = \"../data/Webpages/\"\n",
    "save_path = \"../data/Output/\"\n",
    "index_file = '../webpages/webpages_urls.txt'\n",
    "\n",
    "# # EC2 file directories\n",
    "# source_path = \"/home/ec2-user/s3-drive/Stocks/\"\n",
    "# save_path = \"/home/ec2-user/s3-drive/Output/\"\n",
    "# index_file_path = \"/home/ec2-user/s3-drive/CompanyNames/top_companies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_urls = extract_urls_from_file(index_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_individual_articles_urls(url_list):\n",
    "    \n",
    "    \"\"\"Generates a list of individual articles' URLs that will be processed\n",
    "\n",
    "    Args:\n",
    "        url_list (list): Extracted webpages URLs\n",
    "\n",
    "    Returns:\n",
    "        list: URLs of individual publications.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    HEADERS = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "\n",
    "    config = Config()\n",
    "    config.headers = HEADERS\n",
    "    config.request_timeout = 10\n",
    "    article_urls = []\n",
    "    \n",
    "    for webpage in url_list:\n",
    "        news_portal = newspaper.build(webpage, config = config)#, memoize_articles=False)\n",
    "        for i in range (len(url_list)):\n",
    "            for article in news_portal.articles:\n",
    "                url = article.url\n",
    "                if \"shell\" in url:\n",
    "                    article_urls.append(url)\n",
    "                    \n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(shell_links):\n",
    "        \"\"\"Scrape and transform selected URLs linked to Shell Petroleum for use within the \n",
    "        data processing component of the formed data pipeline.  \n",
    "\n",
    "    Args:\n",
    "        shell_links (list[str]): A list of URLs for publications with Shell info \n",
    "    \"\"\"  \n",
    "    output_data = []\n",
    "    for url in shell_links:\n",
    "        if len(shell_links) >0:\n",
    "            try:\n",
    "                article = Article(url) #pass the url through the Article object\n",
    "                article.download() #download the published article of the url\n",
    "\n",
    "                #NLP preparation\n",
    "                article.parse() #prepare the article for nlp extractions\n",
    "                article.download('punkt') #download 'punkt' to enable tokenization\n",
    "                article.nlp() #enable Natural Language Processing\n",
    "\n",
    "                #Extract html text\n",
    "                html = requests.get(url).text\n",
    "                text = fulltext(html)\n",
    "                \n",
    "                #Filter articles that contains 'oil' in the main text\n",
    "               \n",
    "                if 'oil' in text:\n",
    "                    #Remove HTML tags\n",
    "                    TAG_RE = re.compile(r'<[^>]+>')\n",
    "                    clean_text = TAG_RE.sub('', text)\n",
    "                    new_text = ' '.join(clean_text.splitlines())\n",
    "\n",
    "                    #Remove square brackets\n",
    "                    authors = ','.join(map(str,article.authors))\n",
    "                    keywords = ','.join(map(str,article.keywords))\n",
    "\n",
    "\n",
    "                    output = [article.title,authors,url,new_text,article.summary,keywords,article.publish_date]\n",
    "                    output_data.append(output)\n",
    "\n",
    "            except:\n",
    "                pass   \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(dataframe, output_path, file_name, header):\n",
    "    \"\"\"Saves an input pandas dataframe as a CSV file according to input parameters.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.dataframe): Input dataframe.\n",
    "        output_path (str): Path to which the resulting `.csv` file should be saved. \n",
    "        file_name (str): The name of the output `.csv` file. \n",
    "        header (boolean): Whether to include column headings in the output file.\n",
    "    \"\"\"\n",
    "    print(f\"Path = {output_path}, file = {file_name}\")\n",
    "    dataframe.to_csv(output_path + file_name + \".csv\", index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(extracted_contents, save_folder):\n",
    "    \"\"\"Converts the list of lists to a dataframe and saves into a location.\n",
    "    \n",
    "    Args:\n",
    "        extracted_contents (list[lists[str]]): the processed publications list\n",
    "        save_folder (str): location for storage \n",
    "    \"\"\"\n",
    "    shell_news = pd.DataFrame(extracted_contents)\n",
    "    shell_news.columns = [\"title\",\"authors\",\"url\",\"text\",\"summary\",\"keywords\",\"publish_date\"]\n",
    "    \n",
    "    save_table(shell_news, save_path, 'shell_news_data', header=False)\n",
    "    #return shell_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path = ../data/Output/, file = shell_news_data\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Get all file names in source data directory of companies whose data needs to be processed, \n",
    "    # This information is specified within the `top_companies.txt` file. \n",
    "    web_urls = extract_urls_from_file(index_file)\n",
    "\n",
    "    # Update the company file names to include path information. \n",
    "    individual_articles_urls = get_individual_articles_urls(web_urls)\n",
    "    \n",
    "    data_processing = process_article(individual_articles_urls)\n",
    "\n",
    "    # Process company data and create full data output\n",
    "    convert_to_dataframe(data_processing, save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f057f675914b11b512bf379bae07be6e1618e8fe1362d0973cc146d2f4f584aa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
