# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/data/data_catalog.html
#
# We support interacting with a variety of data stores including local file systems, cloud, network and HDFS
#
# The Data Catalog supports being able to reference the same file using two different DataSet implementations
# (transcoding), templating and a way to reuse arguments that are frequently repeated. See more here:
# https://kedro.readthedocs.io/en/stable/data/data_catalog.html
#
# sdg_data:
#   type: spark.SparkDataSet
#   filepath: data/01_raw/train.csv
#   file_format: csv
#   #credentials: dev_s3
#   load_args:
#     header: True
#     inferSchema: True
#   save_args:
#     sep: '\t'

# rds_data:
#   type: pandas.SQLTableDataSet
#   sql:  SELECT id, "Datetime", "Tweet_Id", "Username", "Verified", "Location", "Reply_Count", "Retweet_Count", "Like_Count", "Quote_Count", text_without_punctuations, hashtags FROM public.twitter_stream
#   credentials: rds_connection
#   layer: raw

model_input_data:
  type: MemoryDataSet

twitter_analytics.dummy_data:
  type: MemoryDataSet

text_comprehension.dummy_data:
  type: MemoryDataSet

text_classification.dummy_data:
  type: MemoryDataSet


# dummy_data:
#   type: spark.SparkDataSet
  # filepath: data/01_raw/osdg-dummy_data.csv
  # file_format: csv
  # #credentials: dev_s3
  # load_args:
  #   header: True
  #   inferSchema: True
  # save_args:
  #   sep: '\t'
  #   overwrite: True



rds_data:
  type: pandas.SQLTableDataSet
  credentials: rds_connection
  table_name: twitter_stream
  save_args:
    if_exists: replace